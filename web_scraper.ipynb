{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Downloading 150 images of 'house centipede' ===\n",
      "[INFO] Renamed images and saved URLs to 'house centipede_urls.csv'.\n",
      "\n",
      "=== Downloading 150 images of 'silverfish' ===\n",
      "[INFO] Renamed images and saved URLs to 'silverfish_urls.csv'.\n",
      "\n",
      "=== Downloading 150 images of 'bedbug' ===\n",
      "[INFO] Renamed images and saved URLs to 'bedbug_urls.csv'.\n",
      "\n",
      "All downloads and CSV exports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# FILE DESCRIPTION: -------------------------------------------------------\n",
    "\n",
    "# This file web scrapes images via Bing. Includes two functions that captures the log as a string, separates invalid\n",
    "# and valid URLs, renames downloaded images in the order they were saved, and writes [image_file_name, URL] to a CSV.\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import io\n",
    "import sys\n",
    "from bing_image_downloader import downloader\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "#\n",
    "# HELPER FUNCTION\n",
    "# ---------------\n",
    "# Given the captured logs (as a single string), separates invalid and valid URLs,\n",
    "# renames downloaded images in the order they were saved, and writes [image_file_name, URL] to a CSV.\n",
    "#\n",
    "def process_download_logs(\n",
    "    logs_str,              # entire captured log as a string\n",
    "    download_folder,       # e.g. \"centipede/centipede\"\n",
    "    csv_file,              # e.g. \"centipede_urls.csv\"\n",
    "    rename_suffix          # e.g. \"centipede\" for \"image_1_centipede.jpg\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Parses the Bing Image Downloader logs to extract valid URLs, \n",
    "    renames the images, and writes the mapping to a CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert log string to list of lines\n",
    "    logs = logs_str.splitlines()\n",
    "\n",
    "    valid_urls = []\n",
    "    invalid_urls = set()\n",
    "\n",
    "    # Identify invalid URLs from lines\n",
    "    # (Bing image downloader logs typically show \"[Error]Invalid image\" or \"Issue getting:\")\n",
    "    for line in logs:\n",
    "        if \"[Error]Invalid image\" in line or \"Issue getting:\" in line:\n",
    "            url_start = line.find(\"http\")\n",
    "            if url_start != -1:\n",
    "                invalid_url = line[url_start:].strip()\n",
    "                invalid_urls.add(invalid_url)\n",
    "\n",
    "    # Extract valid URLs (not in invalid_urls)\n",
    "    for line in logs:\n",
    "        if \"http\" in line:\n",
    "            url_start = line.find(\"http\")\n",
    "            if url_start != -1:\n",
    "                clean_url = line[url_start:].strip()\n",
    "                if clean_url not in invalid_urls:\n",
    "                    valid_urls.append(clean_url)\n",
    "\n",
    "    # Make sure the download folder exists\n",
    "    if not os.path.exists(download_folder):\n",
    "        print(f\"[ERROR] Download folder '{download_folder}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Sort files by creation time so earliest are considered first\n",
    "    downloaded_files = os.listdir(download_folder)\n",
    "    downloaded_files = sorted(\n",
    "        downloaded_files,\n",
    "        key=lambda x: os.path.getctime(os.path.join(download_folder, x))\n",
    "    )\n",
    "\n",
    "    # Rename files and write CSV\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"image file name\", \"URL\"])\n",
    "\n",
    "        for index, (filename, url) in enumerate(zip(downloaded_files, valid_urls), start=1):\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "            # new file name: e.g. \"image_1_centipede.jpg\"\n",
    "            new_name = f\"image_{index}_{rename_suffix}{file_extension}\"\n",
    "\n",
    "            original_path = os.path.join(download_folder, filename)\n",
    "            new_path = os.path.join(download_folder, new_name)\n",
    "            os.rename(original_path, new_path)\n",
    "\n",
    "            writer.writerow([new_name, url])\n",
    "\n",
    "    print(f\"[INFO] Renamed images and saved URLs to '{csv_file}'.\")\n",
    "\n",
    "\n",
    "def download_images_and_capture_logs(query, limit, output_dir, adult_filter_off=True, force_replace=False, timeout=60):\n",
    "    \"\"\"\n",
    "    Uses bing_image_downloader to download images and captures logs printed to stdout.\n",
    "    Returns the captured logs as a string.\n",
    "    \"\"\"\n",
    "    buffer = io.StringIO()\n",
    "    # Capture stdout from the downloader\n",
    "    with redirect_stdout(buffer):\n",
    "        downloader.download(\n",
    "            query=query,\n",
    "            limit=limit,\n",
    "            output_dir=output_dir,\n",
    "            adult_filter_off=adult_filter_off,\n",
    "            force_replace=force_replace,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    # Get the entire output\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "# ------------------- MAIN SCRIPT ------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Centipede\n",
    "    print(\"=== Downloading 150 images of 'house centipede' ===\")\n",
    "    logs_centipede = download_images_and_capture_logs(\n",
    "        query=\"house centipede\",\n",
    "        limit=150,\n",
    "        output_dir=\"house centipede\",  # main folder\n",
    "        adult_filter_off=True,\n",
    "        force_replace=False,\n",
    "        timeout=60\n",
    "    )\n",
    "\n",
    "    # Bing Image Downloader typically creates: \"centipede/centipede\"\n",
    "    download_folder_centipede = os.path.join(\"house centipede\", \"house centipede\")\n",
    "    csv_file_centipede = \"house centipede_urls.csv\"\n",
    "\n",
    "    process_download_logs(\n",
    "        logs_str=logs_centipede,\n",
    "        download_folder=download_folder_centipede,\n",
    "        csv_file=csv_file_centipede,\n",
    "        rename_suffix=\"house centipede\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # 2) Silverfish\n",
    "    print(\"\\n=== Downloading 150 images of 'silverfish' ===\")\n",
    "    logs_silverfish = download_images_and_capture_logs(\n",
    "        query=\"silverfish\",\n",
    "        limit=150,\n",
    "        output_dir=\"silverfish\",  # main folder\n",
    "        adult_filter_off=True,\n",
    "        force_replace=False,\n",
    "        timeout=60\n",
    "    )\n",
    "\n",
    "    download_folder_silverfish = os.path.join(\"silverfish\", \"silverfish\")\n",
    "    csv_file_silverfish = \"silverfish_urls.csv\"\n",
    "\n",
    "    process_download_logs(\n",
    "        logs_str=logs_silverfish,\n",
    "        download_folder=download_folder_silverfish,\n",
    "        csv_file=csv_file_silverfish,\n",
    "        rename_suffix=\"silverfish\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3) Bedbug\n",
    "    print(\"\\n=== Downloading 150 images of 'bedbug' ===\")\n",
    "    logs_bedbug = download_images_and_capture_logs(\n",
    "        query=\"bedbug\",\n",
    "        limit=150,\n",
    "        output_dir=\"bedbug\",  # main folder\n",
    "        adult_filter_off=True,\n",
    "        force_replace=False,\n",
    "        timeout=60\n",
    "    )\n",
    "\n",
    "    download_folder_bedbug = os.path.join(\"bedbug\", \"bedbug\")\n",
    "    csv_file_bedbug = \"bedbug_urls.csv\"\n",
    "\n",
    "    process_download_logs(\n",
    "        logs_str=logs_bedbug,\n",
    "        download_folder=download_folder_bedbug,\n",
    "        csv_file=csv_file_bedbug,\n",
    "        rename_suffix=\"bedbug\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nAll downloads and CSV exports completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
