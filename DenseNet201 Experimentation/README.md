### Experiments ran for DenseNet201 to try to improve evaluation metrics
### Test_1: Batch normalization before activation function (ReLU)
### Test_2: Activation function (ReLU) after batch normalization
### Test_3: Tried replicating this diagram that they optimized for DenseNet201 for their research
https://www.researchgate.net/figure/DenseNet-201-model-optimization-where-the-number-of-hidden-layers-are-512-128-64-32_fig3_35504899 Implementing hidden layers that are 512, 128, 64, 32, and ReLU function
