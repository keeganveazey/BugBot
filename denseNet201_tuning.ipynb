{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b45ffaa-4f99-4144-93b8-74cdee700fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 29m 53s]\n",
      "val_accuracy: 0.7840909361839294\n",
      "\n",
      "Best val_accuracy So Far: 0.7909091114997864\n",
      "Total elapsed time: 10h 10m 52s\n",
      "Epoch 1/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 857ms/step - accuracy: 0.4590 - loss: 1.6527 - val_accuracy: 0.7409 - val_loss: 0.8454\n",
      "Epoch 2/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 850ms/step - accuracy: 0.7905 - loss: 0.6823 - val_accuracy: 0.7500 - val_loss: 0.7995\n",
      "Epoch 3/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 851ms/step - accuracy: 0.8312 - loss: 0.5310 - val_accuracy: 0.7614 - val_loss: 0.7649\n",
      "Epoch 4/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 852ms/step - accuracy: 0.8715 - loss: 0.4386 - val_accuracy: 0.7682 - val_loss: 0.7260\n",
      "Epoch 5/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 851ms/step - accuracy: 0.8767 - loss: 0.3996 - val_accuracy: 0.7864 - val_loss: 0.7439\n",
      "Epoch 6/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 851ms/step - accuracy: 0.8911 - loss: 0.3490 - val_accuracy: 0.7795 - val_loss: 0.7329\n",
      "Epoch 7/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 852ms/step - accuracy: 0.9061 - loss: 0.3140 - val_accuracy: 0.7636 - val_loss: 0.7684\n",
      "Epoch 8/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 852ms/step - accuracy: 0.9026 - loss: 0.3193 - val_accuracy: 0.7682 - val_loss: 0.7675\n",
      "Epoch 9/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 853ms/step - accuracy: 0.9069 - loss: 0.2884 - val_accuracy: 0.7682 - val_loss: 0.7538\n",
      "Epoch 10/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 851ms/step - accuracy: 0.9046 - loss: 0.2904 - val_accuracy: 0.7841 - val_loss: 0.7468\n",
      "best parameters:\n",
      " {'best_lr': 0.001, 'best_dropout': 0.30000000000000004, 'best_batch_size': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shirleyfong/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 789ms/step - accuracy: 0.7873 - loss: 0.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.681067705154419, test_acc: 0.7727272510528564\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0, VGG16, DenseNet201\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import HyperParameters \n",
    "\n",
    "\n",
    "# ----------- CONSTANTS ----------------\n",
    "\n",
    "# define directory structure\n",
    "TRAIN_DIR = \"PROCESSED_DATA/TRAINING_DATA/TRAINING_AUGMENTED_DATA\"\n",
    "VALID_DIR = \"PROCESSED_DATA/VALIDATION_DATA/\"\n",
    "TEST_DIR = \"PROCESSED_DATA/TEST_DATA/\"\n",
    "\n",
    "# Image Parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NORMALIZE_FLAG = True\n",
    "NO_FRILLS_DATAGEN = ImageDataGenerator()\n",
    "NORM_DATAGEN = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "def load_data(directory,shuffle_flag=True):\n",
    "    '''\n",
    "    Param: \n",
    "        - directory - str, \n",
    "        - shuffle_flag - boolean, introduces constrolled stochasticity\n",
    "    '''\n",
    "    if NORMALIZE_FLAG == True:\n",
    "        generator = NORM_DATAGEN.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',shuffle=shuffle_flag)\n",
    "        return generator\n",
    "    else:\n",
    "        generator = NO_FRILLS_DATAGEN.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',shuffle=shuffle_flag)\n",
    "        return generator\n",
    "\n",
    "TRAIN_GENERATOR = load_data(TRAIN_DIR)\n",
    "VAL_GENERATOR = load_data(VALID_DIR)\n",
    "TEST_GENERATOR = load_data(TEST_DIR,shuffle_flag=False)\n",
    "hp = HyperParameters()\n",
    "\n",
    "def build_transfer_learning(hp):\n",
    "    '''\n",
    "    builds a transfer learning model using denseNet201 with classification layers\n",
    "    specified in the JutePestDetect paper: https://arxiv.org/pdf/2308.05179 \n",
    "    \n",
    "    classification portion includes:\n",
    "    -global average pooling layer\n",
    "    -instead of 30% dropout layer, using tunable dropout\n",
    "    -dense layer w softmax classifier\n",
    "    '''\n",
    "    base_model = DenseNet201(weights='imagenet', include_top=False,input_shape=(224, 224, 3))\n",
    "    \n",
    "    # freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    dropout_rate = hp.Float(\"dropout\", min_value=0.2, max_value=0.5, step=0.1)\n",
    "    \n",
    "    # add global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # dense layer w softmax classifier\n",
    "    output_layer = Dense(TRAIN_GENERATOR.num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "    \n",
    "    # tune\n",
    "    learning_rate = hp.Choice('lr', values=[1e-2, 1e-3, 1e-4])\n",
    "    batch_size = hp.Choice('batch_size', values=[16, 32])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "def build_best_model_transfer_learning():\n",
    "\n",
    "    # Define the Bayesian tuner\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_transfer_learning,\n",
    "        objective='val_accuracy',  # tune by improving validation accuracy\n",
    "        max_trials=20,  # num different hp combos to try\n",
    "        executions_per_trial=1,  # run each model once\n",
    "        directory='bayesian_tuning',\n",
    "        project_name='lr_and_drop_tuning_DenseNet201'\n",
    "    )\n",
    "    \n",
    "    # search hp combos\n",
    "    tuner.search(TRAIN_GENERATOR, validation_data=VAL_GENERATOR, epochs=10)\n",
    "    \n",
    "    # get best hps\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    # save them\n",
    "    best_hps_dict = {'best_lr': best_hps.get('lr'),\n",
    "                     'best_dropout': best_hps.get('dropout'),\n",
    "                     'best_batch_size': best_hps.get('batch_size')}\n",
    "    \n",
    "    # make final model with the best drop out, learning rate and batch size\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    best_model_training_history = best_model.fit(TRAIN_GENERATOR, validation_data=VAL_GENERATOR, epochs=10, batch_size=best_hps.get('batch_size'))\n",
    "    \n",
    "    return best_hps_dict, best_model, best_model_training_history\n",
    "\n",
    "\n",
    "def evaluate_model(model, filename = \"best_model_densenet201.h5\"):\n",
    "    '''\n",
    "    Saves model to h5 file, returns test accuracy loss and test accuracy\n",
    "    '''\n",
    "    # evaluate on test data\n",
    "    test_loss, test_acc = model.evaluate(TEST_GENERATOR)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # save to file\n",
    "    model.save(filename)\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # best model\n",
    "    best_hps_dict, best_model, best_model_training_history = build_best_model_transfer_learning()\n",
    "\n",
    "    print(f'best parameters:\\n {best_hps_dict}')\n",
    "    \n",
    "    test_loss, test_acc = evaluate_model(best_model)\n",
    "    print(f'test_loss: {test_loss}, test_acc: {test_acc}')\n",
    "    \n",
    "    best_model.save(\"densenet201_best_model_bayes_optimization.h5\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
